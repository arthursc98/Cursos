{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"language_modelling\"></a>\n",
    "<h2>What exactly is Language Modelling?</h2>\n",
    "Language Modelling, to put it simply, <b>is the task of assigning probabilities to sequences of words</b>. This means that, given a context of one or a sequence of words in the language the model was trained on, the model should provide the next most probable words or sequence of words that follows from the given sequence of words the sentence. Language Modelling is one of the most important tasks in Natural Language Processing.\n",
    "\n",
    "<img src=\"https://ibm.box.com/shared/static/1d1i5gub6wljby2vani2vzxp0xsph702.png\" width=\"1080\">\n",
    "<center><i>Example of a sentence being predicted</i></center>\n",
    "<br><br>\n",
    "In this example, one can see the predictions for the next word of a sentence, given the context \"This is an\". As you can see, this boils down to a sequential data analysis task -- you are given a word or a sequence of words (the input data), and, given the context (the state), you need to find out what is the next word (the prediction). This kind of analysis is very important for language-related tasks such as <b>Speech Recognition, Machine Translation, Image Captioning, Text Correction</b> and many other very relevant problems. \n",
    "\n",
    "<img src=\"https://ibm.box.com/shared/static/az39idf9ipfdpc5ugifpgxnydelhyf3i.png\" width=\"1080\">\n",
    "<center><i>The above example is a schema of an RNN in execution</i></center>\n",
    "<br><br>\n",
    "As the above image shows, Recurrent Network models fit this problem like a glove. Alongside LSTM and its capacity to maintain the model's state for over one thousand time steps, we have all the tools we need to undertake this problem. The goal for this notebook is to create a model that can reach <b>low levels of perplexity</b> on our desired dataset.\n",
    "\n",
    "For Language Modelling problems, <b>perplexity</b> is the way to gauge efficiency. Perplexity is simply a measure of how well a probabilistic model is able to predict its sample. A higher-level way to explain this would be saying that <b>low perplexity means a higher degree of trust in the predictions the model makes</b>. Therefore, the lower perplexity is, the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"word_embedding\"></a>\n",
    "<h2>Word Embeddings</h2><br/>\n",
    "\n",
    "For better processing, in this example, we will make use of <a href=\"https://www.tensorflow.org/tutorials/word2vec/\"><b>word embeddings</b></a>, which is <b>a way of representing sentence structures or words as n-dimensional vectors (where n is a reasonably high number, such as 200 or 500) of real numbers</b>. Basically, we will assign each word a randomly-initialized vector, and input those into the network to be processed. After a number of iterations, these vectors are expected to assume values that help the network to correctly predict what it needs to -- in our case, the probable next word in the sentence. This is shown to be a very effective task in Natural Language Processing, and is a commonplace practice.\n",
    "<br><br>\n",
    "<font size=\"4\"><strong>\n",
    "$$Vec(\"Example\") = [0.02, 0.00, 0.00, 0.92, 0.30, \\ldots]$$\n",
    "</strong></font>\n",
    "<br>\n",
    "Word Embedding tends to group up similarly used words <i>reasonably</i> close together in the vectorial space. For example, if we use T-SNE (a dimensional reduction visualization algorithm) to flatten the dimensions of our vectors into a 2-dimensional space and plot these words in a 2-dimensional space, we might see something like this:\n",
    "\n",
    "<img src=\"https://ibm.box.com/shared/static/bqhc5dg879gcoabzhxra1w8rkg3od1cu.png\" width=\"800\">\n",
    "<center><i>T-SNE Mockup with clusters marked for easier visualization</i></center>\n",
    "<br><br>\n",
    "As you can see, words that are frequently used together, in place of each other, or in the same places as them tend to be grouped together -- being closer together the higher they are correlated. For example, \"None\" is pretty semantically close to \"Zero\", while a phrase that uses \"Italy\", you could probably also fit \"Germany\" in it, with little damage to the sentence structure. The vectorial \"closeness\" for similar words like this is a great indicator of a well-built model.\n",
    "\n",
    "<hr>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial weight scale\n",
    "init_scale = 0.1\n",
    "#Initial learning rate\n",
    "learning_rate = 1.0\n",
    "#Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)\n",
    "max_grad_norm = 5\n",
    "#The number of layers in our model\n",
    "num_layers = 2\n",
    "#The total number of recurrence steps, also known as the number of layers when our RNN is \"unfolded\"\n",
    "num_steps = 20\n",
    "#The number of processing units (neurons) in the hidden layers\n",
    "hidden_size_l1 = 256\n",
    "hidden_size_l2 = 128\n",
    "#The maximum number of epochs trained with the initial learning rate\n",
    "max_epoch_decay_lr = 4\n",
    "#The total number of epochs in training\n",
    "max_epoch = 15\n",
    "#The probability for keeping data in the Dropout Layer (This is an optimization, but is outside our scope for this notebook!)\n",
    "#At 1, we ignore the Dropout Layer wrapping.\n",
    "keep_prob = 1\n",
    "#The decay for the learning rate\n",
    "decay = 0.5\n",
    "#The size for each batch of data\n",
    "batch_size = 60\n",
    "#The size of our vocabulary\n",
    "vocab_size = 10000\n",
    "embeding_vector_size = 200\n",
    "#Training flag to separate training from testing\n",
    "is_training = 1\n",
    "#Data directory for our dataset\n",
    "data_dir = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some clarifications for LSTM architecture based on the arguments:\n",
    "\n",
    "Network structure:\n",
    "<ul>\n",
    "    <li>In this network, the number of LSTM cells are 2. To give the model more expressive power, we can add multiple layers of LSTMs to process the data. The output of the first layer will become the input of the second and so on.\n",
    "    </li>\n",
    "    <li>The recurrence steps is 20, that is, when our RNN is \"Unfolded\", the recurrence step is 20.</li>   \n",
    "    <li>the structure is like:\n",
    "        <ul>\n",
    "            <li>200 input units -> [200x200] Weight -> 200 Hidden units (first layer) -> [200x200] Weight matrix  -> 200 Hidden units (second layer) ->  [200] weight Matrix -> 200 unit output</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "<br>\n",
    "\n",
    "Input layer: \n",
    "<ul>\n",
    "    <li>The network has 200 input units.</li>\n",
    "    <li>Suppose each word is represented by an embedding vector of dimensionality e=200. The input layer of each cell will have 200 linear units. These e=200 linear units are connected to each of the h=200 LSTM units in the hidden layer (assuming there is only one hidden layer, though our case has 2 layers).\n",
    "    </li>\n",
    "    <li>The input shape is [batch_size, num_steps], that is [30x20]. It will turn into [30x20x200] after embedding, and then 20x[30x200]\n",
    "    </li>\n",
    "</ul>\n",
    "<br>\n",
    "\n",
    "Hidden layer:\n",
    "<ul>\n",
    "    <li>Each LSTM has 200 hidden units which is equivalent to the dimensionality of the embedding words and output.</li>\n",
    "</ul>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train data</h4>\n",
    "The story starts from data:\n",
    "<ul>\n",
    "    <li>Train data is a list of words, of size 929589, represented by numbers, e.g. [9971, 9972, 9974, 9975,...]</li>\n",
    "    <li>We read data as mini-batch of size b=30. Assume the size of each sentence is 20 words (num_steps = 20). Then it will take $$floor(\\frac{N}{b \\times h})+1=1548$$ iterations for the learner to go through all sentences once. Where N is the size of the list of words, b is batch size, andh is size of each sentence. So, the number of iterators is 1548\n",
    "    </li>\n",
    "    <li>Each batch data is read from train dataset of size 600, and shape of [30x20]</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Class that represents our model. This class needs a few things:</h4>\n",
    "<ul>\n",
    "    <li>We have to create the model in accordance with our defined hyperparameters</li>\n",
    "    <li>We have to create the placeholders for our input data and expected outputs (the real data)</li>\n",
    "    <li>We have to create the LSTM cell structure and connect them with our RNN structure</li>\n",
    "    <li>We have to create the word embeddings and point them to the input data</li>\n",
    "    <li>We have to create the input structure for our RNN</li>\n",
    "    <li>We have to instantiate our RNN model and retrieve the variable in which we should expect our outputs to appear</li>\n",
    "    <li>We need to create a logistic structure to return the probability of our words</li>\n",
    "    <li>We need to create the loss and cost functions for our optimizer to work, and then create the optimizer</li>\n",
    "    <li>And finally, we need to create a training operation that can be run to actually train our model</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "\n",
    "    def __init__(self, action_type):\n",
    "        ######################################\n",
    "        # Setting parameters for ease of use #\n",
    "        ######################################\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.hidden_size_l1 = hidden_size_l1\n",
    "        self.hidden_size_l2 = hidden_size_l2\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeding_vector_size = embeding_vector_size\n",
    "        ###############################################################################\n",
    "        # Creating placeholders for our input data and expected outputs (target data) #\n",
    "        ###############################################################################\n",
    "        self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n",
    "        self._targets = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n",
    "\n",
    "        ##########################################################################\n",
    "        # Creating the LSTM cell structure and connect it with the RNN structure #\n",
    "        ##########################################################################\n",
    "        # Create the LSTM unit. \n",
    "        # This creates only the structure for the LSTM and has to be associated with a RNN unit still.\n",
    "        # The argument n_hidden(size=200) of BasicLSTMCell is size of hidden layer, that is, the number of hidden units of the LSTM (inside A).\n",
    "        # Size is the same as the size of our hidden layer, and no bias is added to the Forget Gate. \n",
    "        # LSTM cell processes one word at a time and computes probabilities of the possible continuations of the sentence.\n",
    "        lstm_cell_l1 = tf.contrib.rnn.BasicLSTMCell(self.hidden_size_l1, forget_bias=0.0)\n",
    "        lstm_cell_l2 = tf.contrib.rnn.BasicLSTMCell(self.hidden_size_l2, forget_bias=0.0)\n",
    "        \n",
    "        # Unless you changed keep_prob, this won't actually execute -- this is a dropout wrapper for our LSTM unit\n",
    "        # This is an optimization of the LSTM output, but is not needed at all\n",
    "        if action_type == \"is_training\" and keep_prob < 1:\n",
    "            lstm_cell_l1 = tf.contrib.rnn.DropoutWrapper(lstm_cell_l1, output_keep_prob=keep_prob)\n",
    "            lstm_cell_l2 = tf.contrib.rnn.DropoutWrapper(lstm_cell_l2, output_keep_prob=keep_prob)\n",
    "        \n",
    "        # By taking in the LSTM cells as parameters, the MultiRNNCell function junctions the LSTM units to the RNN units.\n",
    "        # RNN cell composed sequentially of multiple simple cells.\n",
    "        stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell_l1, lstm_cell_l2])\n",
    "\n",
    "        # Define the initial state, i.e., the model state for the very first data point\n",
    "        # It initialize the state of the LSTM memory. The memory state of the network is initialized with a vector of zeros and gets updated after reading each word.\n",
    "        self._initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        ####################################################################\n",
    "        # Creating the word embeddings and pointing them to the input data #\n",
    "        ####################################################################\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            # Create the embeddings for our input data. Size is hidden size.\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, self.embeding_vector_size])  #[10000x200]\n",
    "            # Define where to get the data for our embeddings from\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self._input_data)\n",
    "\n",
    "        # Unless you changed keep_prob, this won't actually execute -- this is a dropout addition for our inputs\n",
    "        # This is an optimization of the input processing and is not needed at all\n",
    "        if action_type == \"is_training\" and keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, keep_prob)\n",
    "\n",
    "        ############################################\n",
    "        # Creating the input structure for our RNN #\n",
    "        ############################################\n",
    "        # Input structure is 20x[30x200]\n",
    "        # Considering each word is represended by a 200 dimentional vector, and we have 30 batchs, we create 30 word-vectors of size [30xx2000]\n",
    "        # inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, num_steps, inputs)]\n",
    "        # The input structure is fed from the embeddings, which are filled in by the input data\n",
    "        # Feeding a batch of b sentences to a RNN:\n",
    "        # In step 1,  first word of each of the b sentences (in a batch) is input in parallel.  \n",
    "        # In step 2,  second word of each of the b sentences is input in parallel. \n",
    "        # The parallelism is only for efficiency.  \n",
    "        # Each sentence in a batch is handled in parallel, but the network sees one word of a sentence at a time and does the computations accordingly. \n",
    "        # All the computations involving the words of all sentences in a batch at a given time step are done in parallel. \n",
    "\n",
    "        ####################################################################################################\n",
    "        # Instantiating our RNN model and retrieving the structure for returning the outputs and the state #\n",
    "        ####################################################################################################\n",
    "        \n",
    "        outputs, state = tf.nn.dynamic_rnn(stacked_lstm, inputs, initial_state=self._initial_state)\n",
    "        #########################################################################\n",
    "        # Creating a logistic unit to return the probability of the output word #\n",
    "        #########################################################################\n",
    "        output = tf.reshape(outputs, [-1, self.hidden_size_l2])\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [self.hidden_size_l2, vocab_size]) #[200x1000]\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) #[1x1000]\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "        prob = tf.nn.softmax(logits)\n",
    "        out_words = tf.argmax(prob, axis=2)\n",
    "        self._output_words = out_words\n",
    "        #########################################################################\n",
    "        # Defining the loss and cost functions for the model's learning to work #\n",
    "        #########################################################################\n",
    "            \n",
    "\n",
    "        # Use the contrib sequence loss and average over the batches\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,\n",
    "            self.targets,\n",
    "            tf.ones([batch_size, num_steps], dtype=tf.float32),\n",
    "            average_across_timesteps=False,\n",
    "            average_across_batch=True)\n",
    "    \n",
    "#         loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(self._targets, [-1])],\n",
    "#                                                       [tf.ones([batch_size * num_steps])])\n",
    "        self._cost = tf.reduce_sum(loss)\n",
    "\n",
    "        # Store the final state\n",
    "        self._final_state = state\n",
    "\n",
    "        #Everything after this point is relevant only for training\n",
    "        if action_type != \"is_training\":\n",
    "            return\n",
    "\n",
    "        #################################################\n",
    "        # Creating the Training Operation for our Model #\n",
    "        #################################################\n",
    "        # Create a variable for the learning rate\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        # Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "        tvars = tf.trainable_variables()\n",
    "        # Define the gradient clipping threshold\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self._cost, tvars), max_grad_norm)\n",
    "        # Create the gradient descent optimizer with our learning rate\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        # Create the training TensorFlow Operation through our optimizer\n",
    "        self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    # Helper functions for our LSTM RNN class\n",
    "\n",
    "    # Assign the learning rate for this model\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "    # Returns the input data for this model at a point in time\n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "\n",
    "\n",
    "    \n",
    "    # Returns the targets for this model at a point in time\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    # Returns the initial state for this model\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    # Returns the defined Cost\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    # Returns the final state for this model\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "    \n",
    "    # Returns the final output words for this model\n",
    "    @property\n",
    "    def final_output_words(self):\n",
    "        return self._output_words\n",
    "    \n",
    "    # Returns the current learning rate for this model\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    # Returns the training operation defined for this model\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "# run_one_epoch takes as parameters the current session, the model instance, the data to be fed, and the operation to be run #\n",
    "##########################################################################################################################\n",
    "def run_one_epoch(session, m, data, eval_op, verbose=False):\n",
    "\n",
    "    #Define the epoch size based on the length of the data, batch size and the number of steps\n",
    "    epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "\n",
    "    state = session.run(m.initial_state)\n",
    "    \n",
    "    #For each step and data point\n",
    "    for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size, m.num_steps)):\n",
    "        \n",
    "        #Evaluate and return cost, state by running cost, final_state and the function passed as parameter\n",
    "        cost, state, out_words, _ = session.run([m.cost, m.final_state, m.final_output_words, eval_op],\n",
    "                                     {m.input_data: x,\n",
    "                                      m.targets: y,\n",
    "                                      m.initial_state: state})\n",
    "\n",
    "        #Add returned cost to costs (which keeps track of the total costs for this epoch)\n",
    "        costs += cost\n",
    "        \n",
    "        #Add number of steps to iteration counter\n",
    "        iters += m.num_steps\n",
    "\n",
    "        if verbose and step % (epoch_size // 10) == 10:\n",
    "            print(\"Itr %d of %d, perplexity: %.3f speed: %.0f wps\" % (step , epoch_size, np.exp(costs / iters), iters * m.batch_size / (time.time() - start_time)))\n",
    "\n",
    "    # Returns the Perplexity rating for us to keep track of how the model is evolving\n",
    "    return np.exp(costs / iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, _, _ = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-4-bd6b308d57ff>:27: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-4-bd6b308d57ff>:38: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From C:\\Users\\Arthur\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-4-bd6b308d57ff>:76: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\Arthur\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 4620.359 speed: 947 wps\n",
      "Itr 87 of 774, perplexity: 1282.663 speed: 1015 wps\n",
      "Itr 164 of 774, perplexity: 982.441 speed: 1022 wps\n",
      "Itr 241 of 774, perplexity: 815.448 speed: 1055 wps\n",
      "Itr 318 of 774, perplexity: 720.853 speed: 1077 wps\n",
      "Itr 395 of 774, perplexity: 644.606 speed: 1088 wps\n",
      "Itr 472 of 774, perplexity: 585.221 speed: 1097 wps\n",
      "Itr 549 of 774, perplexity: 531.661 speed: 1105 wps\n",
      "Itr 626 of 774, perplexity: 488.976 speed: 1111 wps\n",
      "Itr 703 of 774, perplexity: 455.035 speed: 1115 wps\n",
      "Epoch 1 : Train Perplexity: 430.813\n",
      "Epoch 1 : Valid Perplexity: 258.389\n",
      "Epoch 2 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 277.037 speed: 1101 wps\n",
      "Itr 87 of 774, perplexity: 239.935 speed: 1126 wps\n",
      "Itr 164 of 774, perplexity: 229.788 speed: 1144 wps\n",
      "Itr 241 of 774, perplexity: 220.322 speed: 1151 wps\n",
      "Itr 318 of 774, perplexity: 217.625 speed: 1154 wps\n",
      "Itr 395 of 774, perplexity: 211.749 speed: 1156 wps\n",
      "Itr 472 of 774, perplexity: 207.452 speed: 1158 wps\n",
      "Itr 549 of 774, perplexity: 200.713 speed: 1159 wps\n",
      "Itr 626 of 774, perplexity: 195.191 speed: 1159 wps\n",
      "Itr 703 of 774, perplexity: 191.027 speed: 1160 wps\n",
      "Epoch 2 : Train Perplexity: 188.317\n",
      "Epoch 2 : Valid Perplexity: 177.444\n",
      "Epoch 3 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 189.170 speed: 1156 wps\n",
      "Itr 87 of 774, perplexity: 161.515 speed: 1133 wps\n",
      "Itr 164 of 774, perplexity: 157.709 speed: 1089 wps\n",
      "Itr 241 of 774, perplexity: 153.244 speed: 1078 wps\n",
      "Itr 318 of 774, perplexity: 153.306 speed: 1069 wps\n",
      "Itr 395 of 774, perplexity: 150.493 speed: 1061 wps\n",
      "Itr 472 of 774, perplexity: 148.997 speed: 1046 wps\n",
      "Itr 549 of 774, perplexity: 145.160 speed: 1032 wps\n",
      "Itr 626 of 774, perplexity: 142.298 speed: 1022 wps\n",
      "Itr 703 of 774, perplexity: 140.426 speed: 1015 wps\n",
      "Epoch 3 : Train Perplexity: 139.343\n",
      "Epoch 3 : Valid Perplexity: 153.520\n",
      "Epoch 4 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 150.174 speed: 918 wps\n",
      "Itr 87 of 774, perplexity: 128.466 speed: 981 wps\n",
      "Itr 164 of 774, perplexity: 126.512 speed: 1014 wps\n",
      "Itr 241 of 774, perplexity: 123.325 speed: 1024 wps\n",
      "Itr 318 of 774, perplexity: 123.991 speed: 1010 wps\n",
      "Itr 395 of 774, perplexity: 122.065 speed: 996 wps\n",
      "Itr 472 of 774, perplexity: 121.200 speed: 984 wps\n",
      "Itr 549 of 774, perplexity: 118.398 speed: 979 wps\n",
      "Itr 626 of 774, perplexity: 116.448 speed: 975 wps\n",
      "Itr 703 of 774, perplexity: 115.354 speed: 979 wps\n",
      "Epoch 4 : Train Perplexity: 114.806\n",
      "Epoch 4 : Valid Perplexity: 142.532\n",
      "Epoch 5 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 127.321 speed: 950 wps\n",
      "Itr 87 of 774, perplexity: 109.573 speed: 1041 wps\n",
      "Itr 164 of 774, perplexity: 108.240 speed: 1014 wps\n",
      "Itr 241 of 774, perplexity: 105.606 speed: 995 wps\n",
      "Itr 318 of 774, perplexity: 106.360 speed: 983 wps\n",
      "Itr 395 of 774, perplexity: 104.884 speed: 978 wps\n",
      "Itr 472 of 774, perplexity: 104.349 speed: 974 wps\n",
      "Itr 549 of 774, perplexity: 102.053 speed: 971 wps\n",
      "Itr 626 of 774, perplexity: 100.535 speed: 980 wps\n",
      "Itr 703 of 774, perplexity: 99.754 speed: 987 wps\n",
      "Epoch 5 : Train Perplexity: 99.473\n",
      "Epoch 5 : Valid Perplexity: 136.374\n",
      "Epoch 6 : Learning rate: 0.500\n",
      "Itr 10 of 774, perplexity: 108.869 speed: 1043 wps\n",
      "Itr 87 of 774, perplexity: 93.410 speed: 1055 wps\n",
      "Itr 164 of 774, perplexity: 91.563 speed: 1034 wps\n",
      "Itr 241 of 774, perplexity: 88.587 speed: 1008 wps\n",
      "Itr 318 of 774, perplexity: 88.798 speed: 1018 wps\n",
      "Itr 395 of 774, perplexity: 86.995 speed: 1022 wps\n",
      "Itr 472 of 774, perplexity: 86.184 speed: 1007 wps\n",
      "Itr 549 of 774, perplexity: 83.784 speed: 1001 wps\n",
      "Itr 626 of 774, perplexity: 82.135 speed: 996 wps\n",
      "Itr 703 of 774, perplexity: 81.122 speed: 999 wps\n",
      "Epoch 6 : Train Perplexity: 80.578\n",
      "Epoch 6 : Valid Perplexity: 125.959\n",
      "Epoch 7 : Learning rate: 0.250\n",
      "Itr 10 of 774, perplexity: 94.998 speed: 961 wps\n",
      "Itr 87 of 774, perplexity: 82.274 speed: 972 wps\n",
      "Itr 164 of 774, perplexity: 80.725 speed: 961 wps\n",
      "Itr 241 of 774, perplexity: 78.059 speed: 959 wps\n",
      "Itr 318 of 774, perplexity: 78.276 speed: 958 wps\n",
      "Itr 395 of 774, perplexity: 76.638 speed: 958 wps\n",
      "Itr 472 of 774, perplexity: 75.848 speed: 963 wps\n",
      "Itr 549 of 774, perplexity: 73.653 speed: 976 wps\n",
      "Itr 626 of 774, perplexity: 72.085 speed: 981 wps\n",
      "Itr 703 of 774, perplexity: 71.052 speed: 979 wps\n",
      "Epoch 7 : Train Perplexity: 70.441\n",
      "Epoch 7 : Valid Perplexity: 124.109\n",
      "Epoch 8 : Learning rate: 0.125\n",
      "Itr 10 of 774, perplexity: 87.572 speed: 920 wps\n",
      "Itr 87 of 774, perplexity: 76.151 speed: 926 wps\n",
      "Itr 164 of 774, perplexity: 74.707 speed: 942 wps\n",
      "Itr 241 of 774, perplexity: 72.257 speed: 951 wps\n",
      "Itr 318 of 774, perplexity: 72.500 speed: 971 wps\n",
      "Itr 395 of 774, perplexity: 70.981 speed: 980 wps\n",
      "Itr 472 of 774, perplexity: 70.223 speed: 974 wps\n",
      "Itr 549 of 774, perplexity: 68.157 speed: 972 wps\n",
      "Itr 626 of 774, perplexity: 66.656 speed: 971 wps\n",
      "Itr 703 of 774, perplexity: 65.627 speed: 970 wps\n",
      "Epoch 8 : Train Perplexity: 65.001\n",
      "Epoch 8 : Valid Perplexity: 123.294\n",
      "Epoch 9 : Learning rate: 0.062\n",
      "Itr 10 of 774, perplexity: 83.528 speed: 974 wps\n",
      "Itr 87 of 774, perplexity: 72.751 speed: 1014 wps\n",
      "Itr 164 of 774, perplexity: 71.378 speed: 1026 wps\n",
      "Itr 241 of 774, perplexity: 69.052 speed: 1014 wps\n",
      "Itr 318 of 774, perplexity: 69.316 speed: 1001 wps\n",
      "Itr 395 of 774, perplexity: 67.881 speed: 991 wps\n",
      "Itr 472 of 774, perplexity: 67.157 speed: 986 wps\n",
      "Itr 549 of 774, perplexity: 65.168 speed: 980 wps\n",
      "Itr 626 of 774, perplexity: 63.703 speed: 985 wps\n",
      "Itr 703 of 774, perplexity: 62.691 speed: 993 wps\n",
      "Epoch 9 : Train Perplexity: 62.068\n",
      "Epoch 9 : Valid Perplexity: 122.776\n",
      "Epoch 10 : Learning rate: 0.031\n",
      "Itr 10 of 774, perplexity: 81.346 speed: 1070 wps\n",
      "Itr 87 of 774, perplexity: 70.922 speed: 987 wps\n",
      "Itr 164 of 774, perplexity: 69.591 speed: 968 wps\n",
      "Itr 241 of 774, perplexity: 67.314 speed: 966 wps\n",
      "Itr 318 of 774, perplexity: 67.590 speed: 961 wps\n",
      "Itr 395 of 774, perplexity: 66.196 speed: 959 wps\n",
      "Itr 472 of 774, perplexity: 65.492 speed: 967 wps\n",
      "Itr 549 of 774, perplexity: 63.545 speed: 978 wps\n",
      "Itr 626 of 774, perplexity: 62.097 speed: 976 wps\n",
      "Itr 703 of 774, perplexity: 61.099 speed: 973 wps\n",
      "Epoch 10 : Train Perplexity: 60.481\n",
      "Epoch 10 : Valid Perplexity: 122.442\n",
      "Epoch 11 : Learning rate: 0.016\n",
      "Itr 10 of 774, perplexity: 80.090 speed: 901 wps\n",
      "Itr 87 of 774, perplexity: 69.902 speed: 948 wps\n",
      "Itr 164 of 774, perplexity: 68.619 speed: 954 wps\n",
      "Itr 241 of 774, perplexity: 66.363 speed: 973 wps\n",
      "Itr 318 of 774, perplexity: 66.646 speed: 989 wps\n",
      "Itr 395 of 774, perplexity: 65.273 speed: 989 wps\n",
      "Itr 472 of 774, perplexity: 64.577 speed: 980 wps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itr 549 of 774, perplexity: 62.653 speed: 973 wps\n",
      "Itr 626 of 774, perplexity: 61.216 speed: 968 wps\n",
      "Itr 703 of 774, perplexity: 60.226 speed: 960 wps\n",
      "Epoch 11 : Train Perplexity: 59.611\n",
      "Epoch 11 : Valid Perplexity: 122.248\n",
      "Epoch 12 : Learning rate: 0.008\n",
      "Itr 10 of 774, perplexity: 79.370 speed: 1058 wps\n",
      "Itr 87 of 774, perplexity: 69.309 speed: 1043 wps\n",
      "Itr 164 of 774, perplexity: 68.070 speed: 1005 wps\n",
      "Itr 241 of 774, perplexity: 65.828 speed: 985 wps\n",
      "Itr 318 of 774, perplexity: 66.116 speed: 977 wps\n",
      "Itr 395 of 774, perplexity: 64.757 speed: 973 wps\n",
      "Itr 472 of 774, perplexity: 64.066 speed: 969 wps\n",
      "Itr 549 of 774, perplexity: 62.156 speed: 966 wps\n",
      "Itr 626 of 774, perplexity: 60.726 speed: 973 wps\n",
      "Itr 703 of 774, perplexity: 59.742 speed: 980 wps\n",
      "Epoch 12 : Train Perplexity: 59.129\n",
      "Epoch 12 : Valid Perplexity: 122.082\n",
      "Epoch 13 : Learning rate: 0.004\n",
      "Itr 10 of 774, perplexity: 78.972 speed: 913 wps\n",
      "Itr 87 of 774, perplexity: 68.967 speed: 946 wps\n",
      "Itr 164 of 774, perplexity: 67.761 speed: 942 wps\n",
      "Itr 241 of 774, perplexity: 65.533 speed: 942 wps\n",
      "Itr 318 of 774, perplexity: 65.825 speed: 941 wps\n",
      "Itr 395 of 774, perplexity: 64.476 speed: 954 wps\n",
      "Itr 472 of 774, perplexity: 63.785 speed: 961 wps\n",
      "Itr 549 of 774, perplexity: 61.884 speed: 958 wps\n",
      "Itr 626 of 774, perplexity: 60.459 speed: 955 wps\n",
      "Itr 703 of 774, perplexity: 59.480 speed: 952 wps\n",
      "Epoch 13 : Train Perplexity: 58.868\n",
      "Epoch 13 : Valid Perplexity: 121.903\n",
      "Epoch 14 : Learning rate: 0.002\n",
      "Itr 10 of 774, perplexity: 78.750 speed: 921 wps\n",
      "Itr 87 of 774, perplexity: 68.771 speed: 923 wps\n",
      "Itr 164 of 774, perplexity: 67.584 speed: 964 wps\n",
      "Itr 241 of 774, perplexity: 65.369 speed: 977 wps\n",
      "Itr 318 of 774, perplexity: 65.666 speed: 963 wps\n",
      "Itr 395 of 774, perplexity: 64.324 speed: 955 wps\n",
      "Itr 472 of 774, perplexity: 63.634 speed: 950 wps\n",
      "Itr 549 of 774, perplexity: 61.738 speed: 947 wps\n",
      "Itr 626 of 774, perplexity: 60.317 speed: 945 wps\n",
      "Itr 703 of 774, perplexity: 59.340 speed: 952 wps\n",
      "Epoch 14 : Train Perplexity: 58.730\n",
      "Epoch 14 : Valid Perplexity: 121.752\n",
      "Epoch 15 : Learning rate: 0.001\n",
      "Itr 10 of 774, perplexity: 78.625 speed: 951 wps\n",
      "Itr 87 of 774, perplexity: 68.656 speed: 965 wps\n",
      "Itr 164 of 774, perplexity: 67.479 speed: 966 wps\n",
      "Itr 241 of 774, perplexity: 65.274 speed: 962 wps\n",
      "Itr 318 of 774, perplexity: 65.577 speed: 961 wps\n",
      "Itr 395 of 774, perplexity: 64.241 speed: 957 wps\n",
      "Itr 472 of 774, perplexity: 63.553 speed: 972 wps\n",
      "Itr 549 of 774, perplexity: 61.659 speed: 982 wps\n",
      "Itr 626 of 774, perplexity: 60.240 speed: 989 wps\n",
      "Itr 703 of 774, perplexity: 59.265 speed: 987 wps\n",
      "Epoch 15 : Train Perplexity: 58.656\n",
      "Epoch 15 : Valid Perplexity: 121.660\n",
      "Test Perplexity: 117.615\n"
     ]
    }
   ],
   "source": [
    "# Initializes the Execution Graph and the Session\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    initializer = tf.random_uniform_initializer(-init_scale, init_scale)\n",
    "    \n",
    "    # Instantiates the model for training\n",
    "    # tf.variable_scope add a prefix to the variables created with tf.get_variable\n",
    "    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "        m = PTBModel(\"is_training\")\n",
    "        \n",
    "    # Reuses the trained parameters for the validation and testing models\n",
    "    # They are different instances but use the same variables for weights and biases, they just don't change when data is input\n",
    "    with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "        mvalid = PTBModel(\"is_validating\")\n",
    "        mtest = PTBModel(\"is_testing\")\n",
    "\n",
    "    #Initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        # Define the decay for this epoch\n",
    "        lr_decay = decay ** max(i - max_epoch_decay_lr, 0.0)\n",
    "        \n",
    "        # Set the decayed learning rate as the learning rate for this epoch\n",
    "        m.assign_lr(session, learning_rate * lr_decay)\n",
    "\n",
    "        print(\"Epoch %d : Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "        \n",
    "        # Run the loop for this epoch in the training model\n",
    "        train_perplexity = run_one_epoch(session, m, train_data, m.train_op, verbose=True)\n",
    "        print(\"Epoch %d : Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "        \n",
    "        # Run the loop for this epoch in the validation model\n",
    "        valid_perplexity = run_one_epoch(session, mvalid, valid_data, tf.no_op())\n",
    "        print(\"Epoch %d : Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "    \n",
    "    # Run the loop in the testing model to see how effective was our training\n",
    "    test_perplexity = run_one_epoch(session, mtest, test_data, tf.no_op())\n",
    "    \n",
    "    print(\"Test Perplexity: %.3f\" % test_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
